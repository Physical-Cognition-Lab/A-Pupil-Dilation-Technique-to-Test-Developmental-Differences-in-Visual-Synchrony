# Estimate the density of p-values
Density = estimate_density(Tt, method = 'kernel')
# Find the maximum density point
MaxP = Density[which.max(Density$y), ]$x
# Create the plot
ggplot(Density, aes(x = x, y = y)) +
geom_line(color = '#7f8dc4') +
geom_ribbon(aes(ymin = 0, ymax = y), fill = '#7f8dc4', alpha = 9) +
geom_point(x = MaxP, y = max(Density$y), size = 4, color = 'black', fill = '#5C6EB4', pch = 21) +
annotate("text", x = MaxP * 1.5, y = max(Density$y), label = as.character(round(MaxP, 4)), color = 'black', size = 9) +
geom_segment(x = 0.05, xend = 0.05, y = 0, yend = 10, linetype = 'dashed', color = 'darkred', lwd = 1.4) +
geom_point(x = 0.05, y = 10, color = 'darkred', size = 4) +
annotate("text", x = 0.05, y = 10.5, label = '0.05', color = 'black', size = 9) +
theme_minimal(base_size = 40) +
labs(x = 'P-value', y = 'Density')
library(tidyverse)
library(zoo)
# Functions ---------------------------------------------------------------
# RollingT function performs a t-test on rolling windows of the data.
# It returns the p-value and t-statistic, or NA if the threshold of NA values is exceeded.
RollingT = function(df) {
df = as.data.frame(df)
Child_window = as.numeric(df$Children)
Adu_window = as.numeric(df$Adult)
# Check for NA values in each window and compare with threshold
if (sum(is.na(Child_window)) / length(Child_window) > 0.75 || sum(is.na(Adu_window)) / length(Adu_window) > 0.75) {
return(c(Pval = NA, t_value = NA))
} else {
test_result = t.test(Child_window, Adu_window, paired = FALSE, alternative = "two.sided")
return(c(Pval = test_result$p.value, t_value = test_result$statistic))
}
}
# ReadData ----------------------------------------------------------------
Hz = 20
window_size = 4 * Hz  # Set window size based on Hz
# Set working directory
setwd("C:\\Users\\tomma\\Desktop\\Share")
# Read the input data files
db = vroom::vroom('.\\Data\\ProcessedData\\FinalData.csv')
Synch_Pupil = vroom::vroom('.\\Data\\ProcessedData\\Pupil_Synch.csv')
# Rolling TTest Pupil -----------------------------------------------------
# Prepare data: Convert data to wide format with Group as column names
Synch_Pupilupil_Wide = Synch_Pupil %>%
pivot_wider(names_from = Group, values_from = PupilSynch,
id_cols = c("Seconds", "Stimulus", 'Video')) %>%
arrange(Stimulus, Video, Seconds) %>%
ungroup()
# Split data by stimulus for separate processing
data_by_stimulus_P = Synch_Pupilupil_Wide %>%
split(.$Stimulus)
# Apply the RollingT function to each group separately and store results
result_list = lapply(data_by_stimulus_P, function(group_df) {
rollapply(group_df, width = window_size, FUN = function(df) RollingT(df), by.column = FALSE, fill = NA, partial = TRUE)
})
# Combine lists into a single data frame
long_2d_list = do.call(rbind, result_list)
long_2d_list = cbind(ListName = rep(names(result_list), times = sapply(result_list, nrow)), long_2d_list)
# Bind the t-test results back to the original data
TTEST_Pupil = Synch_Pupilupil_Wide %>%
bind_cols(., long_2d_list)
# Adjust p-values using FDR correction and determine significance
TTEST_Pupil = TTEST_Pupil %>%
group_by(Video, Stimulus) %>%
mutate(Pval = as.numeric(Pval),
Pval.fdr = as.numeric(p.adjust(Pval, method = "fdr"))) %>%
ungroup() %>%
mutate(Significance = case_when(
Pval >= 0.05 ~ FALSE,
Pval < 0.05 ~ TRUE,
is.na(Pval) ~ FALSE),
Significance.fdr = case_when(
Pval.fdr >= 0.05 ~ FALSE,
Pval.fdr < 0.05 ~ TRUE,
is.na(Pval.fdr) ~ FALSE))
# Write the final results to a CSV file
write.csv(TTEST_Pupil, '.\\Data\\ProcessedData\\RollingTtest_Pupil.csv', row.names = FALSE)
library(tidyverse)
library(parallel)
library(zoo)
# Read Data, functions and Settings ------------------------------------------------
Hz = 20
window_sizeP = Hz * 2  # Window size for calculating rolling correlation
# Set working directory
setwd("C:\\Users\\tomma\\Desktop\\Share")
# Load running average person correlation functions
source(".\\Scripts\\RunningSynchrony.R")
library(tidyverse)
library(parallel)
library(zoo)
# Read Data, functions and Settings ------------------------------------------------
Hz = 20
window_sizeP = Hz * 2  # Window size for calculating rolling correlation
# Set working directory
setwd("C:\\Users\\tomma\\Desktop\\Share")
# Load running average person correlation functions
source(".\\Scripts\\0_SynchronyFunctions.R")
# Read the input data
df = read.csv('.\\Data\\ProcessedData\\FinalData.csv', sep = ',')
# Cluster Preparation ----------------------------------------------------------
iterations = 1:100
# Initialize parallel processing
cl = makeCluster(detectCores() / 2) # Use half of available cores
# Export necessary objects and functions to each worker
clusterExport(cl, c("df", "calculate_running_correlation", "average_fisher_z_transform", "window_sizeP"))
# Load required libraries in each worker
clusterEvalQ(cl, {
library(tidyverse)
library(zoo)
})
# Run the Cluster ---------------------------------------------------------
# Function to run synchronization analysis for each iteration
MultipleSynhc = function(x) {
# Randomize the group labels for each subject
db = df %>%
group_by(Subject) %>%
mutate(Group = sample(c('Adults', 'Children'), size = n(), replace = TRUE)) %>%
ungroup()
#### Adults Group Processing
Adults = db %>%
filter(Group == 'Adults') %>%
arrange(Stimulus) %>%
pivot_wider(names_from = Subject, values_from = Pupil,
id_cols = c("Seconds", "Stimulus", "Video"))
ByStimulus = Adults %>% split(.$Stimulus)
ByStimulus = lapply(ByStimulus, function(df) df %>% select(-1:-3))
Res = lapply(ByStimulus, calculate_running_correlation, window_sizeP = window_sizeP)
Adults = Adults %>%
mutate(PupilSynch = unlist(Res, use.names = FALSE),
Group = 'Adults') %>%
select(Seconds, Group, Video, Stimulus, PupilSynch)
#### Children Group Processing
Children = db %>%
filter(Group == 'Children') %>%
arrange(Stimulus) %>%
pivot_wider(names_from = Subject, values_from = Pupil,
id_cols = c("Seconds", "Stimulus", "Video"))
ByStimulus = Children %>% split(.$Stimulus)
ByStimulus = lapply(ByStimulus, function(df) df %>% select(-1:-3))
Res = lapply(ByStimulus, calculate_running_correlation, window_sizeP = window_sizeP)
Children = Children %>%
mutate(PupilSynch = unlist(Res, use.names = FALSE),
Group = 'Children') %>%
select(Seconds, Group, Video, Stimulus, PupilSynch)
#### Combine Adults and Children Results
Synch_Pupil = bind_rows(Children, Adults) %>%
mutate(Iteration = x)
return(Synch_Pupil)
}
# Run the MultipleSynhc function in parallel for each iteration
DB = parLapply(cl, iterations, MultipleSynhc)
stopCluster(cl) # Stop the cluster after processing is complete
# Combine the results from all iterations
DB = bind_rows(DB)
# Save the final result as an RDS file
saveRDS(DB, '.\\Data\\ProcessedData\\Bootstrap\\PupilSynch_Boot.rds')
library(tidyverse)
library(parallel)
library(zoo)
# Functions ---------------------------------------------------------------
# Define the RollingT function
RollingT = function(df) {
df = as.data.frame(df)
Child_window = as.numeric(df$Children)
Adu_window = as.numeric(df$Adult)
# Check for NA values in each window
ifelse(sum(is.na(Child_window)) / length(Child_window) > 0.75 || sum(is.na(Adu_window)) / length(Adu_window) > 0.75,
NA,
t.test(Child_window, Adu_window, paired = FALSE, alternative = "two.sided")$p.value)
}
# ReadData ----------------------------------------------------------------
Hz = 20
window_size = 4*Hz
l
library(tidyverse)
library(parallel)
library(zoo)
# Functions ---------------------------------------------------------------
# Define the RollingT function
RollingT = function(df) {
df = as.data.frame(df)
Child_window = as.numeric(df$Children)
Adu_window = as.numeric(df$Adult)
# Check for NA values in each window
ifelse(sum(is.na(Child_window)) / length(Child_window) > 0.75 || sum(is.na(Adu_window)) / length(Adu_window) > 0.75,
NA,
t.test(Child_window, Adu_window, paired = FALSE, alternative = "two.sided")$p.value)
}
# ReadData ----------------------------------------------------------------
Hz = 20
window_size = 4*Hz
l
library(tidyverse)
library(parallel)
library(zoo)
# Functions ---------------------------------------------------------------
# Define the RollingT function
RollingT = function(df) {
df = as.data.frame(df)
Child_window = as.numeric(df$Children)
Adu_window = as.numeric(df$Adult)
# Check for NA values in each window
ifelse(sum(is.na(Child_window)) / length(Child_window) > 0.75 || sum(is.na(Adu_window)) / length(Adu_window) > 0.75,
NA,
t.test(Child_window, Adu_window, paired = FALSE, alternative = "two.sided")$p.value)
}
# ReadData ----------------------------------------------------------------
Hz = 20
window_size = 4*Hz
setwd("C:\\Users\\tomma\\Desktop\\Share")
# Read data
Synch_Pupil = readRDS('.\\Data\\ProcessedData\\Bootstrap\\PupilSynch_Boot.rds')
# split dataframe in list based iterators
Synch_Pupil_list = split(Synch_Pupil, Synch_Pupil$Iteration)
# Rolling TTest Pupil ------------------------------------------------------------
ApplyRollingT = function(DF) {
Iter = DF$Iteration[1]
# Prepare data
Synch_Pupilupil_Wide = DF %>%
pivot_wider(names_from = Group, values_from = PupilSynch,
id_cols = c("Seconds", "Stimulus", 'Video')) %>%
arrange(Stimulus, Video,  Seconds) %>%
ungroup()
# Split data by stimulus
data_by_stimulus_P = Synch_Pupilupil_Wide %>%
split(.$Stimulus)
# Apply the RollingT function to each group separately
result_list = lapply(data_by_stimulus_P, function(group_df) {
rollapply(group_df, width = window_size, FUN = function(df) RollingT(df), by.column = FALSE, fill = NA, partial = TRUE)
})
# Combine the results back into a single data frame
TTEST_Pupil = Synch_Pupilupil_Wide %>%
bind_cols( ., stack(result_list)) %>%
rename(Pval = values) %>%
select(-ind) %>%
group_by(Video,Stimulus)%>%
mutate(Pval.fdr =   p.adjust(Pval, method = "fdr"))%>%
ungroup()%>%
mutate( Significance = case_when(
Pval >= 0.05 ~ FALSE,
Pval < 0.05 ~ TRUE,
is.na(Pval) ~ FALSE),
Significance.fdr = case_when(
Pval.fdr >= 0.05 ~ FALSE,
Pval.fdr < 0.05 ~ TRUE,
is.na(Pval.fdr) ~ FALSE))
TTEST_Pupil$Iteration = Iter
return(TTEST_Pupil)
}
# Cluster preparation ----------------------------------------------------------
# Use parLapply to process each subject in parallel
cl <- makeCluster(detectCores()/2) # leave 2 core free
# Export necessary objects to each worker
clusterExport(cl, c("ApplyRollingT", "window_size", "RollingT"))
# Load necessary libraries in each worker
clusterEvalQ(cl, {
library(tidyverse)
library(zoo)
})
# Run Cluster -------------------------------------------------------------
DB = parLapply(cl, Synch_Pupil_list, ApplyRollingT)
stopCluster(cl)
DB = bind_rows(DB)
saveRDS(DB, '.\\Data\\ProcessedData\\Bootstrap\\TTEstPupil_Boot.rds')
library(tidyverse)  # Core library for data manipulation and visualization
library(gridExtra)  # Arrange multiple grid-based plots in a grid layout
library(patchwork)  # Combine plots created by ggplot2
library(gt)         # Create presentation-ready tables
library(gtsummary)  # Summary statistics and tables
library(easystats)  # Easystats ecosystem for statistical modeling
# Define a custom theme for gt tables
gt_theme_538 <- function(data, decimals = 3, ...) {
data %>%
opt_all_caps() %>%
opt_table_font(
font = list(
google_font("Chivo"),  # Use the Chivo font from Google Fonts
default_fonts()        # Fall back to default fonts if Chivo is unavailable
)
) %>%
tab_style(
style = cell_borders(
sides = "bottom", color = "transparent", weight = px(2)
),
locations = cells_body(
columns = everything(),
rows = nrow(data$`_data`)  # Apply style to the bottom border of the last row
)
) %>%
tab_options(
column_labels.background.color = "white",  # Set background color for column labels
table.border.top.width = px(3),           # Set top border width for the table
table.border.top.color = "transparent",   # Set top border color for the table
table.border.bottom.color = "transparent", # Set bottom border color for the table
table.border.bottom.width = px(3),        # Set bottom border width for the table
column_labels.border.top.width = px(3),   # Set top border width for column labels
column_labels.border.top.color = "transparent",  # Set top border color for column labels
column_labels.border.bottom.width = px(3),       # Set bottom border width for column labels
column_labels.border.bottom.color = "black",     # Set bottom border color for column labels
data_row.padding = px(3),                       # Set padding for data rows
source_notes.font.size = 12,                    # Set font size for source notes
table.font.size = 16,                           # Set overall table font size
heading.align = "left",                         # Align table heading to the left
...
)
}
# Set the working directory to the location where your data files are stored
setwd("C:\\Users\\tomma\\Desktop\\Share")
# Load the synchronized pupil data and modify the 'Stimulus' column format
Synch_Pupil = vroom::vroom('.\\Data\\ProcessedData\\Pupil_Synch.csv') %>%
mutate(Stimulus = paste(str_sub(Stimulus, 1, -2), ' Chunk', str_sub(Stimulus, -1, -1), sep = ''))
# Load the rolling t-test results and modify the 'Stimulus' column format
Ttest_Pupil = vroom::vroom('.\\Data\\ProcessedData\\RollingTtest_Pupil2.csv') %>%
mutate(Stimulus = paste(str_sub(Stimulus, 1, -2), ' Chunk', str_sub(Stimulus, -1, -1), sep = ''))
# Load the bootstrapped t-test results from an RDS file and modify the 'Stimulus' column format
Ttest_Pupil_Boot = readRDS('.\\Data\\ProcessedData\\Bootstrap\\TTEstPupil_Boot.rds') %>%
mutate(Stimulus = paste(str_sub(Stimulus, 1, -2), ' Chunk', str_sub(Stimulus, -1, -1), sep = ''))
# Load the rolling t-test results and modify the 'Stimulus' column format
Ttest_Pupil = vroom::vroom('.\\Data\\ProcessedData\\RollingTtest_Pupil.csv') %>%
mutate(Stimulus = paste(str_sub(Stimulus, 1, -2), ' Chunk', str_sub(Stimulus, -1, -1), sep = ''))
# Load the bootstrapped t-test results from an RDS file and modify the 'Stimulus' column format
Ttest_Pupil_Boot = readRDS('.\\Data\\ProcessedData\\Bootstrap\\TTEstPupil_Boot.rds') %>%
mutate(Stimulus = paste(str_sub(Stimulus, 1, -2), ' Chunk', str_sub(Stimulus, -1, -1), sep = ''))
## Identify contiguous chunks of significant results
Streak_Pupil = Ttest_Pupil %>%
group_by(Video, Stimulus) %>%
# Create a streak column to identify contiguous significant periods
mutate(Streak = cumsum(!Significance.fdr)) %>%
ungroup()
## Summarize the chunks, focusing on those longer than 2 seconds
Chunks_Pupil = Streak_Pupil %>%
filter(Significance.fdr == TRUE) %>%
group_by(Streak, Stimulus) %>%
summarize(
START = min(Seconds),  # Start time of the significant chunk
END = max(Seconds),    # End time of the significant chunk
DUR = END - START,     # Duration of the chunk
Children = mean(Children, na.rm = TRUE),  # Average children data
Adults = mean(Adults, na.rm = TRUE)       # Average adults data
) %>%
filter(DUR >= 2)  # Retain only chunks with a duration of 2 seconds or more
## Plot the pupil synchrony with highlighted significant chunks
Synch_Pupil %>%
ggplot(aes(x = Seconds, y = PupilSynch, color = Group)) +
geom_rect(
inherit.aes = FALSE, data = Chunks_Pupil,
aes(xmin = START, xmax = END, ymin = -Inf, ymax = Inf),
fill = '#F2D76A', color = 'black'
) +
geom_line(linewidth = 1.2) +
facet_wrap(~Stimulus, ncol = 3, scales = 'free_x') +
theme_minimal(base_size = 25) +
scale_color_manual(values = c("#BD3538", "#455BA6")) +
labs(x = 'Time (s)', y = 'Pupil Synchrony')
## Plot the density distribution of chunk durations
ggplot(Chunks_Pupil, aes(x = DUR)) +
geom_histogram(aes(y = ..ncount..), fill = '#F2D76A', color = 'black', alpha = 0.7, binwidth = 0.25) +
geom_density(aes(y = ..ndensity..), lwd = 1.3, color = 'black', fill = '#F2D76A', alpha = 0.5) +
theme_classic(base_size = 24) +
labs(x = 'Duration (s)', y = 'Density') +
scale_x_continuous(breaks = seq(0, max(Chunks_Pupil$DUR), by = 5))
## Calculate and display the maximum P-value and minimum T-value for significant chunks
Ttest_Pupil %>%
filter(Significance.fdr == TRUE) %>%
summarise(`Max Pvalue` = max(Pval.fdr), `Min Tvalue` = min(t_value.t)) %>%
gt() %>%
tab_header(title = "Max Pvalue and Min Tvalue") %>%
gt_theme_538()
## Summarize chunk duration and count statistics
Chunks_Pupil %>%
group_by(Stimulus) %>%
summarise(
N = n(),  # Number of chunks per stimulus
Mea = mean(DUR)  # Mean duration of chunks per stimulus
) %>%
ungroup() %>%
summarise(
DurationChunks_Mean = mean(Mea),  # Average duration of chunks across stimuli
DurationChunks_Sd = sd(Mea),      # Standard deviation of chunk durations
NumberChunks_Mean = mean(N),      # Average number of chunks across stimuli
NumberChunks_Sd = sd(N)           # Standard deviation of chunk counts
) %>%
pivot_longer(cols = c(DurationChunks_Mean, DurationChunks_Sd, NumberChunks_Mean, NumberChunks_Sd),
names_to = c(".value", "Statistic"), names_sep = "_") %>%
rename(`Duration Chunks` = DurationChunks, `Number Chunks` = NumberChunks) %>%
gt(rowname_col = "Statistic") %>%
tab_header(title = "Duration of chunks") %>%
gt_theme_538()
## Perform ANOVA to test if chunk durations vary significantly by stimulus
TimeChunk = aov(DUR ~ Stimulus, data = Chunks_Pupil)
## Display ANOVA results in a table
as.data.frame(parameters(TimeChunk)) %>%
gt(rowname_col = 'Parameter') %>%
tab_header(title = "Anova on the duration by video chunk") %>%
gt_theme_538()
## Analyze the number of chunks per video and chunk number
ChunksAnalysis = Chunks_Pupil %>%
mutate(
Chunks = as.numeric(str_extract_all(Stimulus, "\\d+")),  # Extract chunk numbers
Video = str_replace_all(Stimulus, "\\d+", "")  # Extract video names
) %>%
group_by(Video, Chunks) %>%
summarize(Number = n())  # Count the number of significant chunks
## Perform ANOVA to see if the number of chunks differs by video or chunk number
NumbAov = aov(Number ~ Video * Chunks, data = ChunksAnalysis)
parameters(NumbAov) %>%
gt(rowname_col = 'Parameter') %>%
tab_header(title = "Anova exploring difference in\nsignificant windows between videos and chunks") %>%
gt_theme_538()
## Identify contiguous chunks of significant results in bootstrapped data
Streak_Pupil_Boot = Ttest_Pupil_Boot %>%
group_by(Iteration, Video, Stimulus) %>%
# Create a streak column to identify contiguous significant periods
mutate(Streak = cumsum(!Significance.fdr)) %>%
ungroup()
## Summarize the chunks, focusing on those longer than 2 seconds
Chunks_Pupil_boot = Streak_Pupil_Boot %>%
filter(Significance.fdr == TRUE) %>%
group_by(Iteration, Streak, Stimulus) %>%
summarize(
START = min(Seconds),  # Start time of the significant chunk
END = max(Seconds),    # End time of the significant chunk
DUR = END - START      # Duration of the chunk
) %>%
filter(DUR >= 2)  # Retain only chunks with a duration of 2 seconds or more
# Define the threshold for similarity between non-random and bootstrapped data
threshold = 0.25
# Add a Counter column to Chunks_Pupil_boot for counting matching chunks
Chunks_Pupil_boot$Counter = 1
Chunks_Pupil_boot$Row = rownames(Chunks_Pupil_boot)  # Store row names for reference
# Initialize the Counter column in Chunks_Pupil
Chunks_Pupil$Counter = 0
# Loop over each row in Chunks_Pupil to identify matching chunks in bootstrapped data
for (A in 1:nrow(Chunks_Pupil)) {
# Select the current row from Chunks_Pupil
row_A = Chunks_Pupil[A,]
# Filter bootstrapped data to only include rows with the same stimulus
Stimulus_condition = Chunks_Pupil_boot[Chunks_Pupil_boot$Stimulus == row_A$Stimulus,]
# Calculate the absolute difference between the start and end times
diff_Start = abs(Stimulus_condition$START - row_A$START)
diff_Stop = abs(Stimulus_condition$END - row_A$END)
# Identify rows where both start and end times are within the threshold
idx = which(diff_Start < threshold & diff_Stop < threshold)
Chunks_Pupil[A, 'Counter'] = length(idx)  # Count the number of matching chunks
}
## Display Significant Chunks also Found in Bootstrapped Data
Chunks_Pupil %>%
ungroup() %>%
arrange(Stimulus, Streak) %>%
gt(rowname_col = "Stimulus") %>%
tab_header(title = "Significant Chunks of Pupil Also Found in the Randomized Data") %>%
gt_theme_538()
# Display chunks from original data that were also found in the bootstrapped data
Chunks_Pupil %>%
ungroup() %>%
select(-Streak, -Adults, -Children) %>%
arrange(Stimulus) %>%
filter(Counter > 0) %>%
gt(rowname_col = 'Stimulus') %>%
tab_header(title = "Significant Chunks of Pupil Also Found in the Randomized Data") %>%
gt_theme_538()
# Prepare data for Wilcoxon test comparing chunk duration between non-random and bootstrapped data
Dur_NonRandom = Chunks_Pupil %>%
mutate(Type = 'Non Random') %>%
select(DUR, Type, Stimulus)
Dur_Random = Chunks_Pupil_boot %>%
group_by(Streak, Stimulus) %>%
summarize(DUR = mean(DUR)) %>%
mutate(Type = 'Random') %>%
select(DUR, Type, Stimulus)
# Perform Wilcoxon test
Wilcx = wilcox.test(Dur_Random$DUR, Dur_NonRandom$DUR)
parameters(Wilcx) %>%
select(W, p) %>%
mutate(Parameter = 'Non-random vs Random') %>%
gt(rowname_col = 'Parameter') %>%
tab_header(title = 'Wilcoxon Test Comparing Duration Between Conditions') %>%
gt_theme_538()
# Analyze and plot the distribution of chunk duration above 2s in bootstrapped data
RandomAbove2 = Chunks_Pupil_boot %>%
ungroup() %>%
mutate(Chunks_above_2s = if_else(DUR > 2, 'Above', 'Below')) %>%
filter(Chunks_above_2s == 'Above') %>%
group_by(Iteration, Chunks_above_2s) %>%
summarize(Number_Of_Chunks_Above_2s = n()) %>%
select(-Chunks_above_2s) %>%
ungroup() %>%
mutate(Iteration = factor(Iteration))
# Display the table summarizing the number of chunks above 2s per iteration
RandomAbove2 %>%
gt(rowname_col = 'Iteration') %>%
tab_header(title = "Number of Chunks Above 2s in the Randomized Data") %>%
gt_theme_538()
# Count and plot the number of 2s chunks in non-random data for comparison
NonR2sch = Chunks_Pupil %>%
ungroup() %>%
filter(DUR >= 2) %>%
summarize(N = n())
# Plot the density of the number of chunks above 2s in the randomized data
ggplot(RandomAbove2, aes(x = Number_Of_Chunks_Above_2s)) +
geom_histogram(aes(y = ..ncount..), fill = '#AF6FB0', color = 'black', alpha = 0.7, binwidth = 1) +
geom_density(aes(y = ..ndensity..), lwd = 1.3, color = 'black', fill = '#AF6FB0', alpha = 0.5) +
geom_line(data = data.frame(x = c(NonR2sch$N, NonR2sch$N), y = c(0, .8)), aes(x = x, y = y),
linewidth = 3, color = '#E0C763') +
geom_hline(yintercept = 0, linewidth = 1.2) +
geom_point(x = NonR2sch$N, y = 0.8, shape = 21, size = 8, color = 'black', fill = '#E0C763') +
geom_text(x = 120, y = .9, size = 8, label = '# of time windows found in\nthe non-randomized data') +
theme_classic(base_size = 25) +
labs(x = 'Number of Significant Time Windows', y = 'Density',
title = 'Density of the Number of Chunks Above 2s in the Randomized Data') +
scale_x_continuous(breaks = seq(0, NonR2sch$N, by = 20))
setwd("C:/Users/tomma/Desktop/Share/A-Pupil-Dilation-Technique-to-Test-Developmental-Differences-in-Visual-Synchrony")
list.files( full.names = FALSE)
